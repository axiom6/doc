

FeedforwardNetworks
  LearningXOR
  GradientBasedLearning
  HiddenUnits
  ArchitectureDesign
  Back-Propagation
    OtherDifferentiationAlgorithms
  HistoricalNotes

Regularize
  Concerns
    Generalization
    Underfitting
    Overfitting,
    Bias
    Variance
  ParameterNormPenalties
  Constrait
    Optimization
    Under-Constrained
  Noise Robustness
  Semi-Supervised Learning
  Multi-Task Learning
  Early Stopping
  Parameter 
    Typing
    Sharing
  Sparse Representations
  Ensemble
    Bagging
    Other
  Dropout
  Adversarial 
    Training
  Tangent 
    Distance
    Prop
    ManifoldClassifier
  Estimators
  
TrainingOptimization
  How Learning Differs from Pure Optimization
  Challenges in Neural Network Optimization
  Basic Algorithms
  Parameter Initialization Strategies
  Algorithms with Adaptive Learning Rates
  Approximate Second-Order Methods
  Optimization Strategies and Meta-Algorithms
  
ConvolutionaNetworks
  The Convolution Operation
  Motivation
  Pooling
  Convolution and Pooling as an Infinitely Strong Prior
  Variants of the Basic Convolution Function
  Structured Outputs
  Data Types
  Efficient Convolution Algorithms
  Random or Unsupervised Features
  The Neuroscientific Basis for Convolutional Networks
  Convolutional Networks and the History of Deep Learning

Sequence Modeling
  Nets
    Recurrent
    Recursive 
  Unfolding Computational Graphs
  Recurrent Neural Networks
  Bidirectional RNNs
  Encoder-Decoder Sequence-to-Sequence Architectures
  Deep Recurrent Networks
  Recursive Neural Networks
  The Challenge of Long-Term Dependencies
  Echo State Networks
  Leaky Units and Other Strategies for Multiple Time Scales
  The Long Short-Term Memory and Other Gated RNNs
  Optimization for Long-Term Dependencies
  Explicit Memory 
   
--- Research ---

Linear Factor Models
  Probabilistic PCA and Factor Analysis
  Independent Component Analysis (ICA)
  Slow Feature Analysis
  Sparse Coding
  Manifold Interpretation of PCA

Autoencoders
  Undercomplete Autoencoders
  Regularized Autoencoders
  Representational Power, Layer Size and Depth
  Stochastic Encoders and Decoders
  Denoising Autoencoders
  Learning Manifolds with Autoencoders
  Contractive Autoencoders
  Predictive Sparse Decomposition
  Applications of Autoencoders

Representation Learning
  Greedy Layer-Wise Unsupervised Pretraining
  Transfer Learning and Domain Adaptation
  Semi-Supervised Disentangling of Causal Factors
  Distributed Representation
  Exponential Gains from Depth
  Providing Clues to Discover Underlying Causes

Structured Probabilistic Models for Deep Learning
  The Challenge of Unstructured Modeling
  Using Graphs to Describe Model Structure
  Sampling from Graphical Models
  Advantages of Structured Modeling
  Learning about Dependencies
  Inference and Approximate Inference
  The Deep Learning Approach to Structured Probabilistic Models

Monte Carlo Methods
  Sampling and Monte Carlo Methods
  Importance Sampling
  Markov Chain Monte Carlo Methods
  Gibbs Sampling
  The Challenge of Mixing between Separated Modes

Confronting the Partition Function
  The Log-Likelihood Gradient
  Stochastic Maximum Likelihood and Contrastive Divergence
  Pseudolikelihood
  Score Matching and Ratio Matching
  Denoising Score Matching
  Noise-Contrastive Estimation
  Estimating the Partition Function

Approximate Inference
  Inference as Optimization
  Expectation Maximization
  MAP Inference and Sparse Coding
  Variational Inference and Learning
  Learned Approximate Inference

BoltzmannMachines
  Restricted
  DeepBeliefNetworks
  Deep
  RealValuedData
  Convolutional
  Other

GenerativeModels
  Back-Propagation through Random Operations
  Directed Generative Nets
  Drawing Samples from Autoencoders
  Generative Stochastic Networks
  Other Generation Schemes
  Evaluating Generative Models
  Conclusion
