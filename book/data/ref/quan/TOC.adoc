
= TOC

== [black]#14 Unsupervised Learning#
=== [black]#14.1 Introduction#
=== [black]#14.2 Association Rules#
       14.2.1 Market Basket Analysis
       14.2.2 The Apriori Algorithm
       14.2.3 Example: Market Basket Analysis
       14.2.4 Unsupervised as Supervised Learning
       14.2.5 Generalized Association Rules
       14.2.6 Choice of Supervised Learning Method
       14.2.7 Example: Market Basket Analysis (Continued)

=== [black]#14.3 Cluster Analysis#
       14.3.1 Proximity Matrices
       14.3.2 Dissimilarities Based on Attributes
       14.3.3 Object Dissimilarity
       14.3.4 Clustering Algorithms
       14.3.5 Combinatorial Algorithms
       14.3.6 K-means
       14.3.7 Gaussian Mixtures as Soft K-means Clustering
       14.3.8 Example: Human Tumor Microarray Data
       14.3.9 Vector Quantization
       14.3.10 K-medoids
       14.3.11 Practical Issues
       14.3.12 Hierarchical Clustering

=== [black]#14.4 Self-Organizing Maps#

=== [black]#14.5 Principal Components#
       14.5.1 Principal Components
       14.5.2 Principal Curves and Surfaces
       14.5.3 Spectral Clustering
       14.5.4 Kernel Principal Components
       14.5.5 Sparse Principal Components

=== [black]#14.6 Non-negative Matrix Factorization#
       14.6.1 Archetypal Analysis

=== [black]#14.7 Independent Component Analysis#
       14.7.0 Exploratory Projection Pursuit
       14.7.1 Latent Variables and Factor Analysis
       14.7.2 Independent Component Analysis
       14.7.3 Exploratory Projection Pursuit
       14.7.4 A Direct Approach to ICA

=== [black]#14.8 Multidimensional Scaling#
=== [black]#14.9 Nonlinear Dimension Reduction#
=== [black]#14.10.Local Multidimensional Scaling#
=== [black]#14.11 The Google PageRank Algorithm#
  Bibliographic Notes
  Exercises

== [black]#2 Supervised Learning#

  2.1 Introduction
  2.2 Variable Types and Terminology
  2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors
      2.3.1 Linear Models and Least Squares
      2.3.2 Nearest-Neighbor Methods
      2.3.3 From Least Squares to Nearest Neighbors
  2.4 Statistical Decision Theory
  2.5 Local Methods in High Dimensions
  2.6 Statistical Models, Supervised Learning and Function Approximation
      2.6.1 A Statistical Model for the Joint Distribution Pr(X, Y )
      2.6.2 Supervised Learning
      2.6.3 Function Approximation
  2.7 Structured Regression Models
      2.7.1 Difficulty of the Problem
  2.8 Classes of Restricted Estimators
      2.8.1 Roughness Penalty and Bayesian Methods
      2.8.2 Kernel Methods and Local Regression
      2.8.3 Basis Functions and Dictionary Methods
  2.9 Model Selection and the Bias–Variance Tradeoff
  Bibliographic Notes
  Exercises

== [black]#3 Regression#
  3.1 Introduction
  3.2 Linear Regression Models and Least Squares
      3.2.1 Example: Prostate Cancer
      3.2.2 The Gauss–Markov Theorem
      3.2.3 Multiple Regression from Simple Univariate Regression
      3.2.4 Multiple Outputs
  3.3 Subset Selection
      3.3.1 Best-Subset Selection
      3.3.2 Forward- and Backward-Stepwise Selection
      3.3.3 Forward-Stagewise Regression
      3.3.4 Prostate Cancer Data Example (Continued)
  3.4 Shrinkage Methods
      3.4.1 Ridge Regression
      3.4.2 The Lasso
      3.4.3 Discussion: Subset Selection, Ridge Regression and the Lasso
      3.4.4 Least Angle Regression
  3.5 Methods Using Derived Input Directions
      3.5.1 Principal Components Regression
      3.5.2 Partial Least Squares
  3.6 Discussion: A Comparison of the Selection and Shrinkage Methods
  3.7 Multiple Outcome Shrinkage and Selection
  3.8 More on the Lasso and Related Path Algorithms
      3.8.1 Incremental Forward Stagewise Regression
      3.8.2 Piecewise-Linear Path Algorithms
      3.8.3 The Dantzig Selector
      3.8.4 The Grouped Lasso
      3.8.5 Further Properties of the Lasso
      3.8.6 Pathwise Coordinate Optimization
  3.9 Computational Considerations

== [black]#4 Linear Methods for Classification#
  4.1 Introduction
  4.2 Linear Regression of an Indicator Matrix
  4.3 Linear Discriminant Analysis
      4.3.1 Regularized Discriminant Analysis
      4.3.2 Computations for LDA
      4.3.3 Reduced-Rank Linear Discriminant Analysis
  4.4 Logistic Regression
      4.4.1 Fitting Logistic Regression Models
      4.4.2 Example: South African Heart Disease
      4.4.3 Quadratic Approximations and Inference
      4.4.4 L1 Regularized Logistic Regression
      4.4.5 Logistic Regression or LDA?
  4.5 Separating Hyperplanes
      4.5.1 Rosenblatt’s Perceptron Learning Algorithm
      4.5.2 Optimal Separating Hyperplanes