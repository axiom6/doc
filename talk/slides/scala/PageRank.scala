import org.apache.spark.SparkContextimport org.apache.spark.SparkContext._object InvertedIndex {  def main(args: Array[String]) = {  val sc = new SparkContext( "local", "Inverted Index")  sc.textFile("data/crawl")    .map { line => val array = line.split("\t", 2)(array(0), array(1)) }    .flatMap {     case (path, text) => text.split("""\W+""") map { word => (word, path) } }    .map {         case (w, p) => ((w, p), 1) }    .reduceByKey { case (n1, n2) => n1 + n2 }    .map {         case ((w, p), n) => (w, (p, n)) }    .groupBy {     case (w, (p, n)) => w }    .map {         case (w, seq) => val seq2 = seq map { case (_, (p, n)) => (p, n) } (w, seq2.mkString(", ")) }    .saveAsTextFile(argz.outpath)  sc.stop() }}